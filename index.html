<!DOCTYPE html>
<html>
<head>
  <title>My General Website</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
    }

    header {
      background-color: #333;
      color: #fff;
      padding: 20px;
      text-align: left;
    }

    section {
      padding: 50px 0;
      text-align: left;
    }

    .section-content {
      font-size: 18px;
      margin-bottom: 40px;
    }

    img {
      max-width: 100%;
      height: auto;
    }
  </style>
</head>
<body>
  <header>
    <h1>Welcome to My General Website</h1>
  </header>

  <section>
    <div class="container">
      <h2 class="section-header">Project Overview</h2>
      <p class="section-content">
        The audio reactive LED display seeks to enhance audio experience by adding visual component. This is done by continously outputing
      
      
      
      </p>
      <img src="image1.jpg" alt="Image 1">
    </div>
  </section>
  
   <section>
    <div class="container">
      <h2 class="section-header">Requirements</h2>
      <p> <i>System characteristics and their justifications.</i> </p>
      <p class="section-content">
        
        <b> Minimal Latency:</b> The time between when the audio is played and when the corresponding data is being displayed should be kept minimal. 
        Based on studies by the International Telecommunication Union (ITU), the threshold for syncronization error detection is about 45 ms to â€“125 ms
        (audio leading display is indicated as a positive value; audio lagging display is indicated as a negative value). This will be the target range.
        <br>
        <br>
        <b>User Control:</b> Different audio (or personal preferences) will require different display attributes. Users must be able to change LED display color,
        display method, brightness, and frequency bins/instruments displayed for each LED strip. To interface with these contorls, a Python based app will be created.
        <br>
        <br>
        <b>Instrument Isolation:</b> Frequency alone is not fully representative of how music is percieved. A more accurate segmentation of audio is done through 
        instruments. Since different instruments can occupy the same frequency bins, Fourier analysis alone can not achieve instrument segmentation/isolation.
        For this purpose, a Convolutional Neural Network will be created using the TensorFlow Python library. Frequency bins will be as an option for display.
        <br>
        <br>
        <b>Modularity:</b> One of the bottlenecks for latency are the digital LED strips. All LED strips are controlled by a single data line and thus
        communication with all the LEDs for each update can take a significant amount of time. To resolve this, a modular system will be create consisting of a
        main controller processing audio data and sending the data to be displayed by other modular display controllers. This allows audio to be sent to only 
        one reciever controller and allows for parallel processing.
        <br>
        <br>
      </p>
      <img src="image1.jpg" alt="Image 1">
    </div>
  </section>
  
  
  <section>
    <div class="container">
      <h2 class="section-header">Audio Processing</h2>
      <p class="section-content">.</p>
      <img src="image1.jpg" alt="Image 1">
    </div>
  </section>
  
  <section>
    <div class="container">
      <h2 class="section-header">Section 2</h2>
      <p class="section-content">This is the content of section 2.</p>
      <img src="image2.jpg" alt="Image 2">
    </div>
  </section>

  <section>
    <div class="container">
      <h2 class="section-header">Section 3</h2>
      <p class="section-content">This is the content of section 3.</p>
      <img src="image3.jpg" alt="Image 3">
    </div>
  </section>

  <!-- Add more sections as needed -->

</body>
</html>
