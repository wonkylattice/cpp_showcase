<!DOCTYPE html>
<html>
<head>
  <title>My General Website</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
    }

    header {
      background-color: #333;
      color: #fff;
      padding: 20px;
      text-align: left;
    }

    section {
      padding: 50px 0;
      text-align: left;
    }

    .section-content {
      font-size: 18px;
      margin-bottom: 40px;
    }

    img {
      max-width: 100%;
      height: auto;
    }
  </style>
</head>
<body>
  <header>
    <h1>Welcome to My General Website</h1>
  </header>

  <section>
    <div class="container">
      <h2 class="section-header">Project Overview</h2>
      <p class="section-content">
        The audio reactive LED display seeks to enhance audio experience by adding visual component. This is done by continously outputing
      
      
      
      </p>
      <img src="image1.jpg" alt="Image 1">
    </div>
  </section>
  
   <section>
    <div class="container">
      <h2 class="section-header"> Requirements
      <p> <i>System characteristics and their justifications.</i> </p>
      </h2>
      
      <p class="section-content">
        
        <b> Minimal Latency:</b> The time between when the audio is played and when the corresponding data is being displayed should be kept minimal. 
        Based on studies by the International Telecommunication Union (ITU), the threshold for syncronization error detection is about 45 ms to â€“125 ms
        (audio leading display is indicated as a positive value; audio lagging display is indicated as a negative value). This will be the target range.
        <br>
        <br>
        <b>User Control:</b> Different audio (or personal preferences) will require different display attributes. Users must be able to change LED display color,
        display method, brightness, and frequency bins/instruments displayed for each LED strip. To interface with these contorls, a Python based app will be created.
        <br>
        <br>
        <b>Instrument Isolation:</b> Frequency alone is not fully representative of how music is percieved. A more accurate segmentation of audio is done through 
        instruments. Since different instruments can occupy the same frequency bins, Fourier analysis alone can not achieve instrument segmentation/isolation.
        For this purpose, a Convolutional Neural Network will be created using the TensorFlow Python library. Frequency bins will be as an option for display.
        <br>
        <br>
        <b>Modularity:</b> One of the bottlenecks for latency are the digital LED strips. All LED strips are controlled by a single data line and thus
        communication with all the LEDs for each update can take a significant amount of time. To resolve this, a modular system will be create consisting of a
        main controller processing audio data and sending the data to be displayed by other modular display controllers. This allows audio to be sent to only 
        one reciever controller and allows for parallel processing.
        <br>
        <br>
      </p>
      <img src="image1.jpg" alt="Image 1">
    </div>
  </section>
  
  
  <section>
    <div class="container">
      <h2 class="section-header">Audio Processing</h2>
      <p class="section-content">.</p>
      <img src="image1.jpg" alt="Image 1">
    </div>
  </section>
  
  <section>
    <div class="container">
      <h2 class="section-header">Python Application</h2>
      <p class="section-content">This is the content of section 2.</p>
      <img src="image2.jpg" alt="Image 2">
    </div>
  </section>

  <section>
    <div class="container">
      <h2 class="section-header"> Instrument Isolation CNN</h2>
      <p class="section-content"> 
        &emsp;The convolutional neural network architecture was based on the following article published in Towards Data Science: <a href="https://towardsdatascience.com/audio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785">Audio AI: isolating vocals from stereo music using Convolutional Neural Networks</a>.
        This model is designed for single instrument isolation and is demonstrated with vocals. The input to the network is a mixed audio magnitude spectogram and its output is a binary mask
        This mask, when applied to the original complex spectogram, isolates the particular instrument.
        
        &emsp; There is a second article discussing the different possibilites for
        expanding this architecture to other instrument groups. 
        <br>
        
        <br>
        &emsp; In the article, a pipeline was created to find mixes and corresponding vocal tracks by scraping Youtube 
        audio and acapella databases. To save time and resources, I chose the free MUSDB18 dataset consisting of 150 .stem files. The available stems were
        full mix, vocals, drums, bass, and others. To follow the article, my first CNN iteration was trained and tested for vocal isolation
      </p>
      
      <h3>Data Preprocessing</h3>
      <p class="section-content">
      &emsp; Each song in the training set, the raw data is normalized and converted to complex spectograms. This was done for both the full mix and for the vocals. From these, a mix without the
      the vocals is created by subtracting the isolated vocals from the full mix. The binary mask is created by setting all frequency bins for which the vocal magntiude
      is larger than than no-vocal mix magnitude to 1.  (db_min)(Assumed). For the input spectogram, if using data augmentation, pitch shifting is applied before converting to the spectogram. The
      mix spectogram and binary mask were converted to the Mel scale. A dB scaling may also be applied at this time.
       <br>
      <img src="images/amin_binary_mask/0.05.png" alt=".05 bm" width="300" height="240"/>
      <img src="images/amin_binary_mask/0.02.png" alt=".02 bm" width="300" height="240"/>
      <img src="images/amin_binary_mask/0.01.png" alt=".01 bm" width="300" height="240"/>
      <img src="images/amin_binary_mask/0.005.png" alt=".005 bm" width="300" height="240"/>
      <img src="images/amin_binary_mask/0.0008.png" alt=".0008 bm" width="300" height="240"/>
      <audio controls>
        <source src="audio/amin_recon_vocals/.01iso.wav" type="audio/wav">
       </audio>
        <br>
              <audio controls>
        <source src="audio/amin_recon_vocals/.01iso.wav" >
       </audio>
        <br>
                      <audio controls>
        <source src="0.01iso.wav" type="audio/wav">
       </audio>
        <br>
        
       &emsp; For training, the input was split into multiple samples using a sliding window of time length (ms), moving along the spectogram one frame at time. The output for each sample was 
       the binary mask's corresponding middle frame of window. This is done to provide enough temporal context to create accurate predicitions. Once split, additional data augmentation 
       such as added noise and frequency masking could be applied. 
      </p>     
      <h3>Variables</h3>
      <p class="section-content"> 
      Dataset Preprocessing Variable:
      <br>
      <b>n_fft:</b> length of STFT window in samples after padding (Typically left at 4096)<br>
       
      <b>hop_size:</b> number of samples between subsequent STFT (Typically left at 256)<br>
      <b>win_length:</b> length of STFT window in samples before 0 padding (Typically left at 1024)<br>
      <b>sr:</b> sampleing rate of audio (Typically downsampled from 44100 to 22050)<br>
      <b>n_mels:</b> number of mel bins (Typically left at 512)<br>
      <b>sample_width:</b> Width of sliding window for indiividal training samples (Left at 25)<br>
      <br>
      <b>output_amin:</b> Lower threshold for dB scaling on output. Higher output_amin signifies<br>
      less non-significant spectrum represented in binary mask.
      <b>input_amin:</b> Lower threshold for dB scaling on input. Higher output_amin signifies<br>
      less non-significant spectrum in input spectogram.
        
      <b>pitch_shift:</b> boolean representing the inclusion of pitch shift data augmentation
      percent noise
      
      Due to the amount of variables present, it was decided use the same STFT values for as the article.
      The varibles that will be varied include the dB minimum, data augmentation variables, and model hyperparameters 
        
      Things that were assumed were the methods for creating the binary mask.
    
      </p>
      <h3> Resutls </h3>
      <p class="section-content"> 
    
      </p>
      <h3Future Improvement and Next Steps</h3>
            <p class="section-content"> 
      dataset better,  data not HQ, create for other isntrucments
    
      </p>
      <img src="image3.jpg" alt="Image 3">
    </div>
  </section>

  <!-- Add more sections as needed -->

</body>
</html>
